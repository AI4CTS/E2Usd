{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from TSpy.view import plot_mulvariate_time_series_and_label\n",
    "from TSpy.label import reorder_label, seg_to_label\n",
    "from TSpy.eval import *\n",
    "from TSpy.utils import len_of_file\n",
    "import scipy.io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'\n",
    "dataset = {'amc_86_01.4d':{'n_segs':4, 'label':{588:0,1200:1,2006:0,2530:2,3282:0,4048:3,4579:2}},\n",
    "        'amc_86_02.4d':{'n_segs':8, 'label':{1009:0,1882:1,2677:2,3158:3,4688:4,5963:0,7327:5,8887:6,9632:7,10617:0}},\n",
    "        'amc_86_03.4d':{'n_segs':7, 'label':{872:0, 1938:1, 2448:2, 3470:0, 4632:3, 5372:4, 6182:5, 7089:6, 8401:0}},\n",
    "        'amc_86_07.4d':{'n_segs':6, 'label':{1060:0,1897:1,2564:2,3665:1,4405:2,5169:3,5804:4,6962:0,7806:5,8702:0}},\n",
    "        'amc_86_08.4d':{'n_segs':9, 'label':{1062:0,1904:1,2661:2,3282:3,3963:4,4754:5,5673:6,6362:4,7144:7,8139:8,9206:0}},\n",
    "        'amc_86_09.4d':{'n_segs':5, 'label':{921:0,1275:1,2139:2,2887:3,3667:4,4794:0}},\n",
    "        'amc_86_10.4d':{'n_segs':4, 'label':{2003:0,3720:1,4981:0,5646:2,6641:3,7583:0}},\n",
    "        'amc_86_11.4d':{'n_segs':4, 'label':{1231:0,1693:1,2332:2,2762:1,3386:3,4015:2,4665:1,5674:0}},\n",
    "        'amc_86_14.4d':{'n_segs':3, 'label':{671:0,1913:1,2931:0,4134:2,5051:0,5628:1,6055:2}},\n",
    "}\n",
    "\n",
    "def dilate_label(label, f, max_len):\n",
    "    slice_list = []\n",
    "    for e in label:\n",
    "        slice_list.append(e*np.ones(f, dtype=int))\n",
    "    return np.concatenate(slice_list)[:max_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate result on MoCap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_on_MoCap():\n",
    "    score_list = []\n",
    "    for fname in dataset:\n",
    "        # data = np.loadtxt('../data/MoCap/4d/'+fname)\n",
    "        label = np.loadtxt('HVGHlearn/MoCap/'+fname+'/001/segm000.txt')[:,0].astype(int)\n",
    "        groundtruth_json = dataset[fname]['label']\n",
    "        groundtruth = seg_to_label(groundtruth_json)\n",
    "        # print(groundtruth.shape)\n",
    "        prediction = reorder_label(dilate_label(label, 100, len(groundtruth)))\n",
    "        f_cut, p_cut, r_cut = evaluate_cut_point(groundtruth, prediction, 100)\n",
    "        ari, anmi, nmi = evaluate_clustering(groundtruth, prediction)\n",
    "        score_list.append(np.array([ari, anmi, nmi, f_cut, p_cut, r_cut]))\n",
    "         # plot_mulvariate_time_series_and_label(data[0].T, label=prediction, groundtruth=groundtruth)\n",
    "        print('ID: %s, ARI: %f, ANMI: %f, NMI: %f,  F1: %f, P: %f, R: %f' %(fname, ari, anmi, nmi, f_cut, p_cut, r_cut))\n",
    "    score_list = np.vstack(score_list)\n",
    "    print('AVG ---- ARI: %f, ANMI: %f, NMI: %f,  F1: %f, P: %f, R: %f' %(np.mean(score_list[:,0])\\\n",
    "        ,np.mean(score_list[:,1])\n",
    "        ,np.mean(score_list[:,2])\n",
    "        ,np.mean(score_list[:,3])\n",
    "        ,np.mean(score_list[:,4])\n",
    "        ,np.mean(score_list[:,5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_on_synthetic():\n",
    "    score_list = []\n",
    "    for i in range(100):\n",
    "        groundtruth = np.loadtxt('../../data/synthetic_data_for_segmentation/test'+str(i)+'.csv', delimiter=',')[:,4].astype(int)    \n",
    "        prediction = np.loadtxt('HVGHlearn/synthetic/test'+str(i)+'/001/segm000.txt')[:,0].astype(int)    \n",
    "        prediction = dilate_label(prediction, 100, len(groundtruth))\n",
    "        f1, precision, recall, ari, ami = evaluation(groundtruth, prediction)\n",
    "        score_list.append(np.array([f1, precision, recall, ari, ami]))\n",
    "        print('ID: %d, F1: %f, Precision: %f, Recall: %f,  ARI: %f, AMI: %f' %(i, f1, precision, recall, ari, ami))\n",
    "    score_list = np.vstack(score_list)\n",
    "    print('AVG ---- F1: %f, Precision: %f, Recall: %f,  ARI: %f, AMI: %f' %(np.mean(score_list[:,0])\\\n",
    "        ,np.mean(score_list[:,1])\n",
    "        ,np.mean(score_list[:,2])\n",
    "        ,np.mean(score_list[:,3])\n",
    "        ,np.mean(score_list[:,4])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on UCR-SEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HVGH refused to give result on some samples\n",
    "# we skip thest samples.\n",
    "def evaluation_on_UCR_SEG():\n",
    "    score_list = []\n",
    "    save_dir = 'HVGHlearn/UCR-SEG/'\n",
    "    dataset_path = '../data/UCR-SEG/UCR_datasets_seg/'\n",
    "    f_list = os.listdir(dataset_path)\n",
    "    for fname in f_list:\n",
    "        info_list = fname[:-4].split('_')\n",
    "        f = info_list[0]\n",
    "        seg_info = {}\n",
    "        i = 0\n",
    "        for seg in info_list[2:]:\n",
    "            seg_info[int(seg)]=i\n",
    "            i+=1\n",
    "        seg_info[len_of_file(dataset_path+fname)]=i\n",
    "        groundtruth = seg_to_label(seg_info)[:-1]\n",
    "        if not os.path.exists(save_dir+fname):\n",
    "            continue\n",
    "        prediction = np.loadtxt('HVGHlearn/UCR-SEG/'+fname+'/001/segm000.txt')[:,0].astype(int)    \n",
    "        prediction = dilate_label(prediction, 50, len(groundtruth))\n",
    "        f1, precision, recall, ari, ami = evaluation(groundtruth, prediction)\n",
    "        score_list.append(np.array([f1, precision, recall, ari, ami]))\n",
    "        print('ID: %s, F1: %f, Precision: %f, Recall: %f,  ARI: %f, AMI: %f' %(f, f1, precision, recall, ari, ami))\n",
    "    score_list = np.vstack(score_list)\n",
    "    print('AVG ---- F1: %f, Precision: %f, Recall: %f,  ARI: %f, AMI: %f' %(np.mean(score_list[:,0])\\\n",
    "        ,np.mean(score_list[:,1])\n",
    "        ,np.mean(score_list[:,2])\n",
    "        ,np.mean(score_list[:,3])\n",
    "        ,np.mean(score_list[:,4])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on ActRecTut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_on_ActRecTut():\n",
    "    score_list = []\n",
    "    dir_list = ['subject1_walk', 'subject2_walk']\n",
    "    for dir_name in dir_list:\n",
    "        dataset_path = os.path.join('../data/','ActRecTut/'+dir_name+'/data.mat')\n",
    "        data = scipy.io.loadmat(dataset_path)\n",
    "        groundtruth = data['labels'].flatten()\n",
    "        prediction = np.loadtxt('HVGHlearn/ActRecTut/'+dir_name+'/001/segm000.txt')[:,0].astype(int)    \n",
    "        prediction = dilate_label(prediction, 100, len(groundtruth))\n",
    "        f1, precision, recall, ari, ami = evaluation(groundtruth, prediction)\n",
    "        score_list.append(np.array([f1, precision, recall, ari, ami]))\n",
    "        print('ID: %s, F1: %f, Precision: %f, Recall: %f,  ARI: %f, AMI: %f' %(dir_name, f1, precision, recall, ari, ami))\n",
    "    score_list = np.vstack(score_list)\n",
    "    print('AVG ---- F1: %f, Precision: %f, Recall: %f,  ARI: %f, AMI: %f' %(np.mean(score_list[:,0])\\\n",
    "        ,np.mean(score_list[:,1])\n",
    "        ,np.mean(score_list[:,2])\n",
    "        ,np.mean(score_list[:,3])\n",
    "        ,np.mean(score_list[:,4])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on PAMAP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_on_PAMAP2():\n",
    "    score_list = []\n",
    "    for i in range(1,10):\n",
    "        groundtruth = np.loadtxt('../../data/PAMAP2/Protocol/subject10'+str(i)+'.dat')[:,1].astype(int)    \n",
    "        prediction = np.loadtxt('HVGHlearn/PAMAP2/subject10'+str(i)+'/001/segm000.txt')[:,0].astype(int)    \n",
    "        prediction = dilate_label(prediction, 100, len(groundtruth))\n",
    "        f_cut, p_cut, r_cut = evaluate_cut_point(groundtruth, prediction, 100)\n",
    "        ari, anmi, nmi = evaluate_clustering(groundtruth, prediction)\n",
    "        score_list.append(np.array([ari, anmi, nmi, f_cut, p_cut, r_cut]))\n",
    "         # plot_mulvariate_time_series_and_label(data[0].T, label=prediction, groundtruth=groundtruth)\n",
    "        print('ID: %d, ARI: %f, ANMI: %f, NMI: %f,  F1: %f, P: %f, R: %f' %(i, ari, anmi, nmi, f_cut, p_cut, r_cut))\n",
    "    score_list = np.vstack(score_list)\n",
    "    print('AVG ---- ARI: %f, ANMI: %f, NMI: %f,  F1: %f, P: %f, R: %f' %(np.mean(score_list[:,0])\\\n",
    "        ,np.mean(score_list[:,1])\n",
    "        ,np.mean(score_list[:,2])\n",
    "        ,np.mean(score_list[:,3])\n",
    "        ,np.mean(score_list[:,4])\n",
    "        ,np.mean(score_list[:,5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def effect_of_length():\n",
    "    time_list = np.loadtxt('HVGHlearn/effect_of_length/time9.txt')\n",
    "    print(time_list)\n",
    "\n",
    "def effect_of_dimension():\n",
    "    time_list = np.loadtxt('HVGHlearn/effect_of_dim/time20.txt')\n",
    "    print(time_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "HVGHlearn/synthetic/test0/001/segm000.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_163995/2726464870.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# evaluation_on_MoCap()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mevaluation_on_synthetic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# evaluation_on_UCR_SEG()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# evaluation_on_ActRecTut()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# evaluation_on_PAMAP2()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_163995/3590275499.py\u001b[0m in \u001b[0;36mevaluation_on_synthetic\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mgroundtruth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/synthetic_data_for_segmentation/test'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HVGHlearn/synthetic/test'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/001/segm000.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdilate_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroundtruth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mari\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mami\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroundtruth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.7/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.7/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    529\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    530\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: HVGHlearn/synthetic/test0/001/segm000.txt not found."
     ]
    }
   ],
   "source": [
    "# evaluation_on_MoCap()\n",
    "evaluation_on_synthetic()\n",
    "# evaluation_on_UCR_SEG()\n",
    "# evaluation_on_ActRecTut()\n",
    "# evaluation_on_PAMAP2()\n",
    "# effect_of_length()\n",
    "# effect_of_dimension()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HVGH",
   "provenance": []
  },
  "interpreter": {
   "hash": "b8bf74ba8fa41333709f927811b6bb106792c1839926d60648de3e47780dee4b"
  },
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
