{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4P6_UboLT5Kq"
   },
   "source": [
    "### Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o9zhZgN5T2XC"
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import time\n",
    "\n",
    "#VAE\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#HDP-GP-HSMM\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Op4cGV1HoGza"
   },
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_jAbb2LUV6Q"
   },
   "outputs": [],
   "source": [
    "class Variational_Auto_Encoder():\n",
    "  def __init__(self, input_dim, hidden_dims, latent_dim, kld_weight, epochs):\n",
    "    self.input_dim = input_dim\n",
    "    self.latent_dim = latent_dim\n",
    "    self.hidden_encoder_dim1 = hidden_dims[0]\n",
    "    self.hidden_encoder_dim2 = hidden_dims[1]\n",
    "    self.hidden_decoder_dim1 = hidden_dims[2]\n",
    "    self.hidden_decoder_dim2 = hidden_dims[3]\n",
    "    self.kld_weight = kld_weight\n",
    "    self.opt = Adam(lr=0.0001)\n",
    "    self.epochs = epochs\n",
    "\n",
    "    #encoder\n",
    "    logvar_prior = tf.keras.Input(shape=(self.latent_dim, ), name='logvar_prior')\n",
    "    mu_prior = tf.keras.Input(shape=(self.latent_dim, ), name='mu_prior')\n",
    "    inputs = tf.keras.layers.Input(shape=(self.input_dim, ), name='encoder_input')\n",
    "    hidden1= tf.keras.layers.Dense(self.hidden_encoder_dim1, activation='relu', name='enc1') (inputs)\n",
    "    hidden2 = tf.keras.layers.Dense(self.hidden_encoder_dim2, activation='relu', name='enc2') (hidden1)\n",
    "    z_mean = tf.keras.layers.Dense(self.latent_dim, activation='linear', name='z_mean')(hidden2)\n",
    "    z_log_var= tf.keras.layers.Dense(self.latent_dim, activation='linear', name='z_log_var')(hidden2)\n",
    "    z = tf.keras.layers.Lambda(self.sampling, name='z')([z_mean, z_log_var])\n",
    "    \n",
    "    enc_outputs = [z_mean, z_log_var, z]\n",
    "    encoder = tf.keras.models.Model(inputs, enc_outputs, name='encoder')\n",
    "\n",
    "    #decoder\n",
    "    latent_inputs = tf.keras.layers.Input(shape=(self.latent_dim,), name='z_sampling')\n",
    "    dec_hidden1 = tf.keras.layers.Dense(self.hidden_decoder_dim1, activation='relu', name='dec1') (latent_inputs)\n",
    "    dec_hidden2 = tf.keras.layers.Dense(self.hidden_decoder_dim2, activation='relu', name='dec2') (dec_hidden1)\n",
    "    outputs = tf.keras.layers.Dense(self.input_dim, activation='sigmoid') (dec_hidden2)\n",
    "\n",
    "    decoder = tf.keras.models.Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "    #VAE(encoder+decoder)\n",
    "    inputs_ = [inputs, logvar_prior, mu_prior]\n",
    "    outputs_ = [ decoder(encoder(inputs)[2]), encoder(inputs)[0], encoder(inputs)[1] , encoder(inputs)[2]]  #output, mu, sigma\n",
    "    self.VAE = tf.keras.models.Model(inputs_, outputs_, name='VAE')\n",
    "    \n",
    "    #Loss\n",
    "    MSE = tf.reduce_sum( tf.math.squared_difference(K.flatten(outputs_[0]), K.flatten(inputs_[0])))\n",
    "    KLD = - 0.5 * tf.reduce_sum(1 + logvar_prior + z_log_var\n",
    "            - (tf.pow(z_mean - mu_prior, 2) \n",
    "            + tf.exp(z_log_var))/tf.exp(logvar_prior))\n",
    "    loss = tf.reduce_mean(MSE + KLD * self.kld_weight )\n",
    "\n",
    "    self.VAE.add_loss(loss)\n",
    "\n",
    "  def sampling(self, args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    \n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "  def compile(self):\n",
    "    self.VAE.compile(optimizer=self.opt)\n",
    "    #print (self.VAE.summary())\n",
    "\n",
    "  def learn(self, data, logvar_prior, mu_prior, verbose=True):\n",
    "    result = self.VAE.fit([data, logvar_prior, mu_prior] , epochs=1, verbose=verbose)\n",
    "    return result\n",
    "\n",
    "  def predict(self, data, logvar_prior, mu_prior, losses=False):\n",
    "    reconst, mu, sigma, z = self.VAE.predict([data, logvar_prior, mu_prior])\n",
    "    return reconst, mu, sigma, z\n",
    "\n",
    "  def plot(self, data, reconst, mu, sigma, z, losses, savepath):\n",
    "    if losses != False:\n",
    "      plt.title('loss')\n",
    "      plt.plot(np.arange(self.epochs), losses)\n",
    "      plt.savefig(savepath+'_loss.png')\n",
    "      plt.close()\n",
    "\n",
    "    plt.title(\"z_alldim\")\n",
    "    plt.plot(np.arange(mu.shape[0]), mu)\n",
    "    plt.savefig(savepath+'_z.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.title(\"z_hat_alldim\")\n",
    "    plt.plot(np.arange(mu.shape[0]), z)\n",
    "    plt.savefig(savepath+'_z_hat.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.title('data_alldim')\n",
    "    plt.plot(np.arange(mu.shape[0]), data)\n",
    "    plt.savefig(savepath+'_oridata.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.title('reconst_alldim')\n",
    "    plt.plot(np.arange(mu.shape[0]), reconst)\n",
    "    plt.savefig(savepath+'_reconst.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8MoFn55VSQ5"
   },
   "source": [
    "### GaussianProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "seXBh3ADUy3T"
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K7NUmCGtU00V"
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cdef extern from \"math.h\":\n",
    "    double exp(double)\n",
    "    double sqrt(double)\n",
    "    double log(double)\n",
    "\n",
    "cdef class GP:\n",
    "    cdef double beta\n",
    "    cdef int ns\n",
    "    cdef xt, yt\n",
    "    cdef double[:,:] i_cov\n",
    "    cdef double[:] param\n",
    "    cdef dict param_cache\n",
    "\n",
    "    cdef double covariance_func(self, double xi, double xj):\n",
    "        cdef double theta0 = 1.0\n",
    "        cdef double theta1 = 1.0\n",
    "        cdef double theta2 = 0\n",
    "        cdef double theta3 = 16.0\n",
    "        return theta0 * exp(-0.5 * theta1 * (xi - xj) * (xi - xj)) + theta2 + theta3 * xi * xj\n",
    "    \n",
    "    cdef double normpdf(self, double x, double mu, double sigma):\n",
    "        return 1./(sqrt(2*np.pi)*sigma)*exp(-0.5 * ((x - mu)/sigma)**2)\n",
    "\n",
    "    def __init__( self ):\n",
    "        self.beta = 10.0\n",
    "        self.param_cache = {}\n",
    "\n",
    "    def learn(self, xt, yt ):\n",
    "        cdef int i, j\n",
    "        self.xt = xt\n",
    "        self.yt = yt\n",
    "        self.ns = len(xt)\n",
    "\n",
    "        cdef double[:,:] cov = np.zeros((self.ns, self.ns))\n",
    "\n",
    "        for i in range(self.ns):\n",
    "            for j in range(self.ns):\n",
    "                cov[i,j] = self.covariance_func(xt[i], xt[j])\n",
    "                if i==j:\n",
    "                    cov[i,j] += 1/self.beta\n",
    "\n",
    "        self.i_cov = np.linalg.inv(cov)\n",
    "        self.param = np.dot(self.i_cov, self.yt)\n",
    "        self.param_cache.clear()\n",
    "\n",
    "    def plot(self, x):\n",
    "        mus, sigmas = self.predict( x.reshape(-1,1) )\n",
    "        plt.plot( x, mus )\n",
    "        \n",
    "        y_max = mus + np.sqrt(sigmas.flatten())\n",
    "        y_min = mus - np.sqrt(sigmas.flatten())\n",
    "\n",
    "        plt.fill_between(x, y_min, y_max, facecolor=\"lavender\" , alpha=0.9 , edgecolor=\"lavender\"  )\n",
    "        plt.plot(self.xt, self.yt)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def predict( self, x ):\n",
    "        mus = []\n",
    "        sigmas = []\n",
    "        n = len(x)\n",
    "        tt = [y - np.random.normal() / self.beta for y in self.yt]\n",
    "        for k in range(n):\n",
    "            v = np.zeros((self.ns))\n",
    "            for i in range(self.ns):\n",
    "                v[i] = self.covariance_func(x[k], self.xt[i])\n",
    "            c = self.covariance_func(x[k], x[k]) + 1.0 / self.beta\n",
    "            \n",
    "            mu = np.dot(v, np.dot(self.i_cov, tt))\n",
    "            sigma = c - np.dot(v, np.dot(self.i_cov, v))\n",
    "            \n",
    "            mus.append(mu)\n",
    "            sigmas.append(sigma)\n",
    "        \n",
    "        return np.array(mus, dtype=float), np.array(sigmas, dtype=float)\n",
    "    \n",
    "\n",
    "    cpdef double calc_lik_al( self, double[:] xs, double[:] ys ):\n",
    "        cdef int k,i\n",
    "        cdef int n = len(xs)\n",
    "        cdef double lik = 0\n",
    "        cdef int ns = self.ns\n",
    "        cdef double c,p,mu,sigma\n",
    "        cdef double[:] v= np.zeros((ns))\n",
    "\n",
    "        for k in range(n):\n",
    "            # 計算結果をキャッシュして使い回す\n",
    "            if xs[k] in self.param_cache:\n",
    "                mu, sigma = self.param_cache[ xs[k] ]\n",
    "            else:\n",
    "                v = np.zeros((ns))\n",
    "                for i in range(ns):\n",
    "                    v[i] = self.covariance_func(xs[k], self.xt[i])\n",
    "                c = self.covariance_func(xs[k], xs[k]) + 1.0 / self.beta\n",
    "                mu = np.dot(v, self.param)\n",
    "                sigma = c - np.dot(v, np.dot(self.i_cov, v))\n",
    "                \n",
    "                self.param_cache[ xs[k] ] = (mu, sigma)\n",
    "\n",
    "            p = self.normpdf( ys[k] , mu, sigma )\n",
    "            if p<=0:\n",
    "                p = 0.000000000001\n",
    "            lik += log( p )\n",
    "\n",
    "        return lik\n",
    "\n",
    "\n",
    "    def calc_lik( self, xs, ys ):\n",
    "      lik = self.calc_lik_al( xs, ys )\n",
    "      return lik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WTHGGvRRaUFb"
   },
   "source": [
    "### logsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tmlzDvM2aV6T"
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.mlab as mlab\n",
    "import sys\n",
    "\n",
    "cdef extern from \"math.h\":\n",
    "    double log(double)\n",
    "    double exp(double)\n",
    "\n",
    "\n",
    "cpdef logsumexp( double[:,:] a ):\n",
    "    cdef double max_val = -sys.float_info.max\n",
    "    cdef double sum_exp = 0\n",
    "    cdef int I = a.shape[0]\n",
    "    cdef int J = a.shape[1]\n",
    "    \n",
    "    for i in range(I):\n",
    "        for j in range(J):\n",
    "            if max_val<a[i,j]:\n",
    "                max_val = a[i,j]\n",
    "                \n",
    "    for i in range(I):\n",
    "        for j in range(J):\n",
    "            sum_exp += exp( a[i,j] - max_val )\n",
    "    return log(sum_exp) + max_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pAbp3HAOWnVW"
   },
   "source": [
    "### multidim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a8FhZCwMWp0d"
   },
   "outputs": [],
   "source": [
    "class GPMD:\n",
    "    def __init__(self, dim):\n",
    "        self.__dim = dim\n",
    "        self.__gp = [ GP() for d in range(self.__dim) ]\n",
    "\n",
    "    def learn(self,x, y ):\n",
    "        y = np.array(y, dtype=float).reshape( (-1,self.__dim) )\n",
    "        x = np.array(x,dtype=float)\n",
    "\n",
    "        for d in range(self.__dim):\n",
    "            if len(y)!=0:\n",
    "                self.__gp[d].learn( x, y[:,d] )\n",
    "            else:\n",
    "                self.__gp[d].learn( x, [] )\n",
    "\n",
    "\n",
    "    def calc_lik(self, x, y, last = False):\n",
    "        lik = 0.0\n",
    "        mus = []\n",
    "        sigmas = []\n",
    "\n",
    "        if self.__dim==1:\n",
    "            y = np.asarray(y, dtype=float).reshape( (-1,self.__dim) )\n",
    "        for d in range(self.__dim):\n",
    "            lik += self.__gp[d].calc_lik( x , y[:,d] )\n",
    "            if last != False:\n",
    "              mu , sig = self.__gp[d].predict(x)\n",
    "              mus.append(mu)\n",
    "              sigmas.append(sig)\n",
    "\n",
    "        if last != False:\n",
    "          return lik, np.array(mus, dtype=float), np.array(sigmas, dtype=float)\n",
    "        else:\n",
    "          return lik\n",
    "\n",
    "    def plot(self, x ):\n",
    "        for d in range(self.__dim):\n",
    "            plt.subplot( self.__dim, 1, d+1 )\n",
    "\n",
    "            mus, sigmas = self.__gp[d].predict(x)\n",
    "            y_min = mus - sigmas*2\n",
    "            y_max = mus + sigmas*2\n",
    "\n",
    "            plt.fill_between( x, y_min, y_max, facecolor=\"lavender\" , alpha=0.9 , edgecolor=\"lavender\"  )\n",
    "            plt.plot(x, y_min, 'b--')\n",
    "            plt.plot(x, mus, 'b-')\n",
    "            plt.plot(x, y_max, 'b--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ibGl4zBWqVg"
   },
   "source": [
    "### segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-0BfwNmBWtc9"
   },
   "outputs": [],
   "source": [
    "class GPSegmentation():\n",
    "    def __init__(self, dim, gamma, alpha, initial_class):\n",
    "        self.dim = dim\n",
    "        self.numclass = initial_class\n",
    "        self.segmlen = 3\n",
    "        self.gps = [ GPMD(dim) for i in range(self.numclass) ]\n",
    "        self.segm_in_class= [ [] for i in range(self.numclass) ]\n",
    "        self.segmclass = {}\n",
    "        self.segments = []\n",
    "        self.trans_prob = np.ones( (1,1) )\n",
    "        self.trans_prob_bos = np.ones( 1 )\n",
    "        self.trans_prob_eos = np.ones( 1 )\n",
    "        self.all_numclass = []\n",
    "        self.counter = 0\n",
    "        self.is_initialized = False\n",
    "        \n",
    "        # parameters\n",
    "        self.MAX_LEN = 20\n",
    "        self.MIN_LEN = 3\n",
    "        self.AVE_LEN = 12\n",
    "        self.SKIP_LEN = 1\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.beta = np.ones(1)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def load_data(self, zs, classfile=None ):\n",
    "        self.data = []\n",
    "        self.segments = []\n",
    "        self.is_initialized = False\n",
    "\n",
    "        for y in zs:\n",
    "            segm = []\n",
    "            self.data.append( np.array(y, dtype=float) )\n",
    "\n",
    "            i = 0\n",
    "            while i<len(y):\n",
    "                length = random.randint(self.MIN_LEN, self.MAX_LEN)\n",
    "\n",
    "                if i+length+1>=len(y):\n",
    "                    length = len(y)-i\n",
    "\n",
    "                segm.append( y[i:i+length+1] )\n",
    "\n",
    "                i+=length\n",
    "\n",
    "            self.segments.append( segm )\n",
    "\n",
    "            for i,s in enumerate(segm):\n",
    "                c = random.randint(0,self.numclass-1)\n",
    "                self.segmclass[id(s) ] = c\n",
    "\n",
    "        self.calc_trans_prob()\n",
    "\n",
    "\n",
    "    def load_model( self, basename ):\n",
    "        for c in range(self.numclass):\n",
    "            filename = basename + \"class%03d.npy\" % c\n",
    "            self.segm_in_class[c] = np.load( filename, allow_pickle=True)\n",
    "            self.update_gp( c )\n",
    " \n",
    "        self.trans_prob = np.load( basename+\"trans.npy\", allow_pickle=True )\n",
    "        self.trans_prob_bos = np.load( basename+\"trans_bos.npy\", allow_pickle=True )\n",
    "        self.trans_prob_eos = np.load( basename+\"trans_eos.npy\", allow_pickle=True )\n",
    "\n",
    "\n",
    "    def update_gp(self, c ):\n",
    "        datay = []\n",
    "        datax = []\n",
    "        for s in self.segm_in_class[c]:\n",
    "            datay += [ y for y in s ]\n",
    "            datax += range(len(s))\n",
    "\n",
    "        self.gps[c].learn( datax, datay )\n",
    "\n",
    "\n",
    "    def calc_emission_logprob( self, c, segm ):\n",
    "        gp = self.gps[c]\n",
    "        slen = len(segm)\n",
    "\n",
    "        if len(segm) > 2:\n",
    "            log_plen = (slen*math.log(self.AVE_LEN) + (-self.AVE_LEN)*math.log(math.e)) - (sum(np.log(np.arange(1,slen+1))))\n",
    "            p = gp.calc_lik( np.arange(len(segm), dtype=float) , segm )\n",
    "            return p + log_plen\n",
    "        else:\n",
    "            return math.log(1.0e-100)\n",
    "\n",
    "    def save_model(self, basename ):\n",
    "        if not os.path.exists(basename):\n",
    "            os.mkdir( basename )\n",
    "\n",
    "        for n,segm in enumerate(self.segments):\n",
    "            classes = []\n",
    "            cut_points = []\n",
    "            for s in segm:\n",
    "                c = self.segmclass[id(s)]\n",
    "                classes += [ c for i in range(len(s)) ]\n",
    "                cut_points += [0] * len(s)\n",
    "                cut_points[-1] = 1\n",
    "            np.savetxt( basename+\"segm%03d.txt\" % n, np.vstack([classes,cut_points]).T, fmt=str(\"%d\") )\n",
    "\n",
    "        for c in range(len(self.gps)):\n",
    "            for d in range(self.dim):\n",
    "                plt.clf()\n",
    "                for data in self.segm_in_class[c]:\n",
    "                    if self.dim==1:\n",
    "                        plt.plot( range(len(data)), data, \"o-\" )\n",
    "                    else:\n",
    "                        plt.plot( range(len(data[:,d])), data[:,d], \"o-\" )\n",
    "                    plt.ylim( -1, 1 )\n",
    "                # plt.savefig( basename+\"class%03d_dim%03d.png\" % (c, d) )\n",
    "                plt.close()\n",
    "\n",
    "        np.save( basename + \"trans.npy\" , self.trans_prob  )\n",
    "        np.save( basename + \"trans_bos.npy\" , self.trans_prob_bos )\n",
    "        np.save( basename + \"trans_eos.npy\" , self.trans_prob_eos )\n",
    "        np.save( basename + \"all_class.npy\", self.segm_in_class[c])\n",
    "\n",
    "        # for c in range(self.numclass):\n",
    "        #     np.save( basename+\"class%03d.npy\" % c, self.segm_in_class[c] )\n",
    "\n",
    "        return self.numclass\n",
    "\n",
    "\n",
    "    def forward_filtering(self, d ):\n",
    "        T = len(d)\n",
    "        log_a = np.log( np.zeros( (len(d), self.MAX_LEN, self.numclass) )  + 1.0e-100 ) \n",
    "        valid = np.zeros( (len(d), self.MAX_LEN, self.numclass) ) \n",
    "        z = np.ones( T )\n",
    "\n",
    "        for t in range(T):\n",
    "            for k in range(self.MIN_LEN,self.MAX_LEN,self.SKIP_LEN):\n",
    "                if t-k<0:\n",
    "                    break\n",
    "\n",
    "                segm = d[t-k:t+1]\n",
    "                for c in range(self.numclass):\n",
    "                    out_prob = self.calc_emission_logprob( c, segm )\n",
    "                    foward_prob = 0.0\n",
    "\n",
    "                    tt = t-k-1\n",
    "                    if tt>=0:\n",
    "                        foward_prob = logsumexp( log_a[tt,:,:] + z[tt] + np.log(self.trans_prob[:,c]) ) + out_prob\n",
    "                    else:\n",
    "                        foward_prob = out_prob + math.log(self.trans_prob_bos[c])\n",
    "\n",
    "                    if t==T-1:\n",
    "                        foward_prob += math.log(self.trans_prob_eos[c])\n",
    "\n",
    "                    log_a[t,k,c] = foward_prob\n",
    "                    valid[t,k,c] = 1.0\n",
    "                    if math.isnan(foward_prob):\n",
    "                        print( \"a[t=%d,k=%d,c=%d] became NAN!!\" % (t,k,c) )\n",
    "                        sys.exit(-1)\n",
    "\n",
    "            if t-self.MIN_LEN>=0:\n",
    "                z[t] = logsumexp( log_a[t,:,:] )\n",
    "                log_a[t,:,:] -= z[t]\n",
    "\n",
    "        return np.exp(log_a)*valid\n",
    "\n",
    "\n",
    "    def sample_idx(self, prob ):\n",
    "        accm_prob = [0,] * len(prob)\n",
    "        for i in range(len(prob)):\n",
    "            accm_prob[i] = prob[i] + accm_prob[i-1]\n",
    "\n",
    "        rnd = random.random() * accm_prob[-1]\n",
    "        for i in range(len(prob)):\n",
    "            if rnd <= accm_prob[i]:\n",
    "                return i\n",
    "\n",
    "\n",
    "    def backward_sampling(self, a, d):\n",
    "        T = a.shape[0]\n",
    "        t = T-1\n",
    "\n",
    "        segm = []\n",
    "        segm_class = []\n",
    "\n",
    "        c = -1\n",
    "        while True:\n",
    "            if t==T-1:\n",
    "                transp = self.trans_prob_eos\n",
    "            else:\n",
    "                transp = self.trans_prob[:,c]\n",
    "            \n",
    "            idx = self.sample_idx( (a[t]*transp).reshape( self.MAX_LEN*self.numclass ))\n",
    "\n",
    "            k = int(idx/self.numclass)\n",
    "            c = idx % self.numclass\n",
    "\n",
    "            if t-k-1<=0:\n",
    "                s = d[0:t+1]\n",
    "            else:\n",
    "                s = d[t-k:t+1]\n",
    "\n",
    "            segm.insert( 0, s )\n",
    "            segm_class.insert( 0, c )\n",
    "\n",
    "            t = t-k-1\n",
    "\n",
    "            if t<=0:\n",
    "                break\n",
    "\n",
    "        return segm, segm_class\n",
    "\n",
    "\n",
    "    def calc_trans_prob( self ):\n",
    "        self.trans_prob = np.zeros( (self.numclass,self.numclass) )\n",
    "        self.trans_prob_bos = np.zeros( self.numclass )\n",
    "        self.trans_prob_eos = np.zeros( self.numclass )\n",
    "\n",
    "        for n,segm in enumerate(self.segments):\n",
    "            if id(segm[0]) in self.segmclass:\n",
    "                c_begin = self.segmclass[ id(segm[0]) ]\n",
    "                self.trans_prob_bos[c_begin]+=1\n",
    "\n",
    "            if id(segm[-1]) in self.segmclass:\n",
    "                c_end = self.segmclass[ id(segm[-1]) ]\n",
    "                self.trans_prob_eos[c_end]+=1\n",
    "\n",
    "            for i in range(1,len(segm)):\n",
    "                try:\n",
    "                    cc = self.segmclass[ id(segm[i-1]) ]\n",
    "                    c = self.segmclass[ id(segm[i]) ]\n",
    "                except KeyError:\n",
    "\n",
    "                    continue\n",
    "                self.trans_prob[cc,c] += 1\n",
    "\n",
    "        self.trans_prob_bos += self.alpha * self.beta\n",
    "        self.trans_prob_eos += self.alpha * self.beta\n",
    "\n",
    "        for c in range(self.numclass):\n",
    "            self.trans_prob[c,:] += self.alpha * self.beta\n",
    "\n",
    "        self.trans_prob = self.trans_prob / self.trans_prob.sum(1).reshape(self.numclass,1)\n",
    "        self.trans_prob_bos = self.trans_prob_bos / np.sum( self.trans_prob_bos )\n",
    "        self.trans_prob_eos = self.trans_prob_eos / np.sum( self.trans_prob_eos )\n",
    "\n",
    "\n",
    "    def sample_num_states(self):\n",
    "\n",
    "        # calculate u\n",
    "        u = []\n",
    "        for n,segm in enumerate(self.segments):\n",
    "            c = self.segmclass[ id(segm[0]) ]\n",
    "            p = self.trans_prob_bos[c]\n",
    "            u.append( random.random() * p )\n",
    "\n",
    "            c = self.segmclass[ id(segm[-1]) ]\n",
    "            p = self.trans_prob_eos[c]\n",
    "            u.append( random.random() * p )\n",
    "\n",
    "            for i in range(1,len(segm)):\n",
    "                cc = self.segmclass[ id(segm[i-1]) ]\n",
    "                c = self.segmclass[ id(segm[i]) ]\n",
    "                p = self.trans_prob[cc,c]\n",
    "                u.append( random.random() * p )\n",
    "\n",
    "        # remove \n",
    "        beta = list( self.beta )\n",
    "        for c in range(self.numclass)[::-1]:\n",
    "            if len(self.segm_in_class[c])==0:\n",
    "                self.numclass -= 1\n",
    "                self.gps.pop()\n",
    "                self.segm_in_class.pop()\n",
    "                beta[-2] += beta[-1]\n",
    "                beta.pop()\n",
    "                #print (\"pop!\")\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        u_min = np.min( u )\n",
    "\n",
    "        N = 0\n",
    "        for c in range(self.numclass):\n",
    "            N += len(self.segm_in_class[c])\n",
    "\n",
    "        while self.alpha*beta[-1]/N > u_min:\n",
    "            stick_len = beta[-1]\n",
    "            rnd = np.random.beta(1,self.gamma)\n",
    "            beta[-1] = stick_len * rnd\n",
    "            beta.append( stick_len * (1-rnd) )\n",
    "            self.numclass += 1\n",
    "            self.gps.append( GPMD(self.dim) )\n",
    "            self.segm_in_class.append([])\n",
    "\n",
    "        self.beta = np.array( beta , dtype=float)\n",
    "\n",
    "        self.all_numclass.append(self.numclass)\n",
    "    \n",
    "    \n",
    "    # If list.remove( elem ), ValueError.\n",
    "    def remove_ndarray(self, lst, elem ):\n",
    "        l = len(elem)\n",
    "        for i,e in enumerate(lst):\n",
    "            if len(e)!=l:\n",
    "                continue\n",
    "            if (e==elem).all():\n",
    "                lst.pop(i)\n",
    "                return\n",
    "        raise ValueError( \"ndarray is not found!!\" )\n",
    "\n",
    "    def learn(self):\n",
    "        if self.is_initialized==False:\n",
    "            # learn GP\n",
    "            for i in range(len(self.segments)):\n",
    "                for s in self.segments[i]:\n",
    "                    c = self.segmclass[id(s)]\n",
    "                    self.segm_in_class[c].append( s )\n",
    "\n",
    "            # learn each classes\n",
    "            for c in range(self.numclass):\n",
    "                self.update_gp( c )\n",
    "\n",
    "            self.is_initialized = True\n",
    "\n",
    "        self.update(True)\n",
    "\n",
    "    def recog(self):\n",
    "        self.update(False)\n",
    "\n",
    "    def update(self, learning_phase=True ):\n",
    "\n",
    "        for i in range(len(self.segments)):\n",
    "            if learning_phase:\n",
    "                print (\"slice sampling\")\n",
    "                self.sample_num_states()\n",
    "            \n",
    "            d = self.data[i]\n",
    "            segm = self.segments[i]\n",
    "\n",
    "            for s in segm:\n",
    "                c = self.segmclass[id(s)]\n",
    "                self.segmclass.pop( id(s) )\n",
    "\n",
    "                if learning_phase:\n",
    "                    # update parameter\n",
    "                    self.remove_ndarray( self.segm_in_class[c], s )\n",
    "\n",
    "            if learning_phase:\n",
    "                # update GP\n",
    "                for c in range(self.numclass):\n",
    "                    self.update_gp( c )\n",
    "\n",
    "                # update transition probability\n",
    "                self.calc_trans_prob()\n",
    "\n",
    "            # start = time.clock()\n",
    "            # print( \"forward...\", end=\"\")\n",
    "            a = self.forward_filtering( d )\n",
    "\n",
    "            # print( \"backward...\", end=\"\" )\n",
    "            segm, segm_class = self.backward_sampling( a, d )\n",
    "            # print( time.clock()-start, \"sec\" )\n",
    "\n",
    "            # print( \"Number of classified segments: [\", end=\"\")\n",
    "            # for s in self.segm_in_class:\n",
    "            #     print( len(s), end=\" \" )\n",
    "            # print( \"]\" )\n",
    "\n",
    "\n",
    "            self.segments[i] = segm\n",
    "\n",
    "            for s,c in zip( segm, segm_class ):\n",
    "                self.segmclass[id(s)] = c\n",
    "\n",
    "                # update parameter\n",
    "                if learning_phase:\n",
    "                    self.segm_in_class[c].append(s)\n",
    "\n",
    "            if learning_phase:\n",
    "                # update GP\n",
    "                for c in range(self.numclass):\n",
    "                    self.update_gp( c )\n",
    "\n",
    "                # update transition probability\n",
    "                self.calc_trans_prob()\n",
    "        return\n",
    "\n",
    "\n",
    "    def calc_lik(self, last=False):\n",
    "        liks = 0\n",
    "        mus_all = []\n",
    "        sigmas_all = []\n",
    "\n",
    "        for segm in self.segments:\n",
    "            # last\n",
    "            if last != False:\n",
    "              mus = [[] for i in range(self.dim)]\n",
    "              sigmas = [[] for i in range(self.dim)]\n",
    "\n",
    "            for n, s in enumerate(segm):\n",
    "                c = self.segmclass[id(s)]\n",
    "                liks += self.gps[c].calc_lik( np.arange(len(s),dtype=float) , np.array(s, dtype=float) )\n",
    "                \n",
    "                # last\n",
    "                if last != False:\n",
    "                  lik, mu, sig = self.gps[c].calc_lik( np.arange(len(s), dtype=float) , s , last)\n",
    "                  if n == 0:\n",
    "                    for dd in range(self.dim):\n",
    "                        mus[dd] = mu[dd]\n",
    "                        sigmas[dd] = sig[dd]\n",
    "                  else:\n",
    "                    for dd in range(self.dim):\n",
    "                        mus[dd] = np.concatenate([mus[dd], mu[dd]])\n",
    "                        sigmas[dd] = np.concatenate([sigmas[dd], sig[dd]])\n",
    "\n",
    "                  liks += lik\n",
    "\n",
    "            # last\n",
    "            if last != False:\n",
    "              mus_all.append((np.array(mus, dtype=float).T).astype(dtype=float))\n",
    "              sigmas_all.append(np.log((np.array(sigmas, dtype=float).T).astype(dtype=float)))\n",
    "\n",
    "        #last\n",
    "        if last != False:\n",
    "          return liks, mus_all, sigmas_all\n",
    "        else:\n",
    "          return liks\n",
    "    \n",
    "    def get_num_class(self):\n",
    "        n = 0\n",
    "        for c in range(self.numclass):\n",
    "            if len(self.segm_in_class[c])!=0:\n",
    "                n += 1\n",
    "        return n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w21E05WtWt66"
   },
   "source": [
    "### main_segm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ws1Ve9DCWv1q"
   },
   "outputs": [],
   "source": [
    "def learn(zs, savedir, dim, gamma, eta, initial_class):\n",
    "    gpsegm = GPSegmentation(dim, gamma, eta, initial_class)\n",
    "\n",
    "    gpsegm.load_data( zs )\n",
    "    liks = []\n",
    "\n",
    "    #iteration (default: 10)\n",
    "    for it in range(5):\n",
    "        print( \"-----\", it, \"-----\" )\n",
    "        gpsegm.learn()\n",
    "        numclass = gpsegm.save_model(savedir)\n",
    "        print(\"lik =\", gpsegm.calc_lik())\n",
    "        liks.append(gpsegm.calc_lik())\n",
    "    #print (\"liks: \",liks)\n",
    "    \n",
    "    #plot liks\n",
    "    # plt.clf()\n",
    "    # plt.plot( range(len(liks)), liks )\n",
    "    # plt.savefig( os.path.join( savedir,\"liks.png\") )\n",
    "\n",
    "    lik, mu, sigma = gpsegm.calc_lik(last=True)\n",
    "    return numclass, np.array(mu, dtype=float), np.array(sigma, dtype=float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created by Chengyu on 2022/1/26.\n",
    "\n",
    "class HVGH():\n",
    "    def __init__(self, epoch=1, iteration=1, gamma=2, eta=2, initial_class=1):\n",
    "        self.epoch = epoch\n",
    "        self.iteration = iteration\n",
    "        self.gamma = gamma\n",
    "        self.eta = eta\n",
    "        self.initial_class = initial_class\n",
    "\n",
    "    def fit(self, dta, save_dir, win_size=20, input_dim=None):\n",
    "        hidden_dim = [40,20,20,40]\n",
    "        latent_dim= 3\n",
    "        \n",
    "        #weight of KLD\n",
    "        kld_weight = 0.9\n",
    "\n",
    "        data_ = []\n",
    "        batch_sizes = []\n",
    "        logvar_priors = []\n",
    "        mu_priors = []\n",
    "\n",
    "        y = dta[::win_size]\n",
    "        data_.append( y )\n",
    "        batch_sizes.append( int(len(y)/4) )\n",
    "\n",
    "        logvar_priors.append( np.array( np.zeros( (len(y),latent_dim) ), dtype=float ) )\n",
    "        mu_priors.append( np.array( np.zeros( (len(y),latent_dim) ), dtype=float ) )\n",
    "\n",
    "        if input_dim is None:\n",
    "            input_dim=len(data_[0][0])\n",
    "        else:\n",
    "            input_dim=input_dim\n",
    "            \n",
    "        #HDP-GP-HSMM parameters\n",
    "        gamma = self.gamma\n",
    "        eta = self.eta\n",
    "        initial_class = self.initial_class\n",
    "\n",
    "        # define VAE\n",
    "        vae = Variational_Auto_Encoder(input_dim, hidden_dim, latent_dim, kld_weight, self.epoch)\n",
    "        vae.compile()\n",
    "\n",
    "        path = ('./')\n",
    "        #learn VAE and HSP-GP-HSMM\n",
    "        for ite in range(self.iteration):\n",
    "            print (\"*--------------iteration:%03d--------------*\"%ite)\n",
    "            zs = []\n",
    "            for n, data in enumerate(data_):\n",
    "                losses = []\n",
    "\n",
    "                for e in range(self.epoch):\n",
    "                    idx = np.random.choice(range(0, len(data)), batch_sizes[n])\n",
    "                    result = vae.learn(data[idx], logvar_priors[n][idx], mu_priors[n][idx], verbose=0)\n",
    "                    losses.append(result.history['loss'])\n",
    "                    \n",
    "                #predicts\n",
    "                reconst, mu, sigma, z = vae.predict(data, logvar_priors[n], mu_priors[n], losses)\n",
    "                savepath = (path+'HVGHlearn/'+save_dir+'/%03d/'%ite)\n",
    "                if not os.path.exists(savepath):\n",
    "                    os.makedirs(savepath)\n",
    "                savepath_ = (savepath + 'data_%03d'%n)\n",
    "                vae.plot(data, reconst, mu, sigma, z, losses, savepath_)\n",
    "                print ('VAE learned', 'iteration:', ite, 'data:', n)\n",
    "\n",
    "                zs.append(np.array(mu))\n",
    "                # np.savetxt(savepath_+'_z.txt', mu)\n",
    "            # vae.VAE.save_weights(savepath+'vae_weights.hdf5')\n",
    "\n",
    "            # HDP-GP-HSMM\n",
    "            z_dim = len(zs[0][0])\n",
    "            recog_initial_class, mu_priors, logvar_priors = learn(zs, savepath, z_dim, gamma, eta, initial_class )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp on MoCap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/'\n",
    "def exp_on_MoCap():\n",
    "    save_dir = 'MoCap/'\n",
    "    dataset_path = data_path + 'MoCap/4d/'\n",
    "    fname_list = os.listdir(dataset_path)\n",
    "    fname_list = list(fname_list)\n",
    "    fname_list.sort()\n",
    "    for fname in fname_list[1:]:\n",
    "        full_path = dataset_path + fname\n",
    "        print(full_path)\n",
    "        hvgh = HVGH(epoch=5, iteration=2)\n",
    "        data = np.loadtxt(full_path, dtype=float)\n",
    "        hvgh.fit(data, save_dir+fname, win_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp on Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_on_synthetic():\n",
    "    save_dir = 'synthetic/'\n",
    "    dataset_path = data_path + 'synthetic_data_for_segmentation/'\n",
    "    \n",
    "    for i in range(100):\n",
    "        fname = 'test'+str(i)+'.csv'\n",
    "        full_path = dataset_path + fname\n",
    "        data = np.loadtxt(full_path, dtype=float, delimiter=',')[:,:4]\n",
    "        hvgh = HVGH(epoch=5, iteration=2)\n",
    "        hvgh.fit(data, save_dir+'test'+str(i), win_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp on UCR-SEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_on_UCR_SEG():\n",
    "    save_dir = 'UCR-SEG/'\n",
    "    dataset_path = data_path + 'UCR-SEG/UCR_datasets_seg/'\n",
    "    # 21-23, 0-3\n",
    "    f_list = os.listdir(dataset_path)\n",
    "    f_list.sort()\n",
    "    for fname in f_list[30:]:\n",
    "        print(fname)\n",
    "        full_path = dataset_path+fname\n",
    "        data = np.loadtxt(full_path)\n",
    "        hvgh = HVGH(epoch=10, iteration=10)\n",
    "        hvgh.fit(data, save_dir+fname, input_dim=1, win_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp on ActRecTut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_on_ActRecTut():\n",
    "    save_dir = 'ActRecTut/'\n",
    "    dataset_path = data_path + 'ActRecTut/'\n",
    "    # dir_list = ['subject1_walk', 'subject2_walk']\n",
    "    dir_list = ['subject2_walk']\n",
    "    for dir_name in dir_list:\n",
    "        full_path = dataset_path+dir_name+'/data.mat'\n",
    "        mat = scipy.io.loadmat(full_path)\n",
    "        # There are bugs in HVGH, use the full length will\n",
    "        # lead to failures. Thus we cut the length to 30000.\n",
    "        data = mat['data'][:,:10]\n",
    "        print(data.shape)\n",
    "        hvgh = HVGH(epoch=5, iteration=2)\n",
    "        hvgh.fit(data, save_dir+dir_name, win_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp on PAMAP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nan(data):\n",
    "    x_len, y_len = data.shape\n",
    "    for x in range(x_len):\n",
    "        for y in range(y_len):\n",
    "            if np.isnan(data[x,y]):\n",
    "                data[x,y]=data[x-1,y]\n",
    "    return data\n",
    "\n",
    "def exp_on_PAMAP2():\n",
    "    save_dir = 'PAMAP2/'\n",
    "    score_list = []\n",
    "    for i in range(1,9):\n",
    "        dataset_path = '../../data/PAMAP2/Protocol/subject10'+str(i)+'.dat'\n",
    "        data = np.loadtxt(dataset_path)\n",
    "        hand_acc = data[:,4:7]\n",
    "        chest_acc = data[:,21:24]\n",
    "        ankle_acc = data[:,38:41]\n",
    "        data = np.hstack([hand_acc, chest_acc, ankle_acc])\n",
    "        data = fill_nan(data)\n",
    "\n",
    "        hvgh = HVGH(epoch=10, iteration=2)\n",
    "        hvgh.fit(data, save_dir+'subject10'+str(i), win_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp on USC-HAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TSpy.dataset import *\n",
    "\n",
    "def exp_on_USC_HAD():\n",
    "    save_dir = 'USC-HAD/'\n",
    "    for subject in range(1,15):\n",
    "        for target in range(1,6):\n",
    "            data, groundtruth = load_USC_HAD(subject, target, data_path)\n",
    "            try:\n",
    "                hvgh = HVGH(epoch=5, iteration=2)\n",
    "                hvgh.fit(data, save_dir+'subject'+str(subject)+'_target'+str(target), win_size=100)\n",
    "            except:\n",
    "                print('Failed to segment s%d_t%d'%(subject, target))\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def effect_of_length():\n",
    "    time_list = []\n",
    "    save_dir='effect_of_length/'\n",
    "\n",
    "    data = np.loadtxt('../../data/synthetic_data_for_segmentation/test0.csv', delimiter=',')\n",
    "    data = np.concatenate([data[:,:4] for x in range(15)])\n",
    "    # print(data.shape)\n",
    "    for length in range(1,21):\n",
    "        hvgh = HVGH(epoch=10, iteration=2)\n",
    "        time_start=time.time()\n",
    "        hvgh.fit(data[:length*10000,:], save_dir, win_size=15)\n",
    "        time_end=time.time()\n",
    "        print('totally cost',time_end-time_start)\n",
    "        time_list.append(time_end-time_start)\n",
    "        np.savetxt('HVGHlearn/'+save_dir+'time'+str(length)+'.txt', np.array(time_list).round(2))\n",
    "\n",
    "    time_list = np.array(time_list)\n",
    "    print(time_list.round(2))\n",
    "\n",
    "def effect_of_dimension():\n",
    "    time_list = []\n",
    "    save_dir='effect_of_dim/'\n",
    "    #31392\n",
    "    data = np.loadtxt('../../data/synthetic_data_for_segmentation/test0.csv', delimiter=',')\n",
    "    data = np.hstack([data, data, data, data, data])\n",
    "    data = np.vstack([data, data])\n",
    "    data = data[:31392,:]\n",
    "    print(data.shape)\n",
    "    for i in range(1,20):\n",
    "        hvgh = HVGH(epoch=10, iteration=2)\n",
    "        time_start=time.time()\n",
    "        hvgh.fit(data[:30000, :i], save_dir, win_size=15)\n",
    "        time_end=time.time()\n",
    "        print('totally cost',time_end-time_start)\n",
    "        time_list.append(time_end-time_start)\n",
    "        np.savetxt('HVGHlearn/'+save_dir+'time'+str(i)+'.txt', np.array(time_list).round(2))\n",
    "\n",
    "    time_list = np.array(time_list)\n",
    "    print(time_list.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_on_MoCap()\n",
    "# exp_on_synthetic()\n",
    "# exp_on_UCR_SEG()\n",
    "# exp_on_ActRecTut()\n",
    "# exp_on_PAMAP2()\n",
    "exp_on_USC_HAD()\n",
    "# effect_of_length()\n",
    "# effect_of_dimension()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HVGH",
   "provenance": []
  },
  "interpreter": {
   "hash": "b8bf74ba8fa41333709f927811b6bb106792c1839926d60648de3e47780dee4b"
  },
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
